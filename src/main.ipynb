{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'util' from 'util/__init__.pyc'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(level = logging.DEBUG , format=\n",
    "        '%(asctime)s:%(levelname)s:%(name)s:%(threadName)s:line %(lineno)d: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level = logging.INFO , format=\n",
    "        '%(asctime)s:%(levelname)s:%(name)s:%(threadName)s:line %(lineno)d: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import cPickle\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Embedding, Conv1D, GlobalMaxPool1D, Dense, GlobalAvgPool1D, Dropout\n",
    "from keras.layers import concatenate\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import sequence\n",
    "from keras import regularizers\n",
    "from keras.engine.topology import Layer\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "\n",
    "from util import plot_model, plot_metric, save_code, fill_dict\n",
    "from util.archiver import get_archiver\n",
    "import config as c\n",
    "\n",
    "MAX_CHORDS = None\n",
    "MAX_LABELS = None\n",
    "NUM_NOTES = 88\n",
    "NUM_DIM = 1024\n",
    "\n",
    "M1 = M2 = W = b2 = None\n",
    "data= train= test= valid= MAX_CHORDS = None\n",
    "labels= y_train= y_test= y_valid= MAX_LABELS= index2label= labels2index = None\n",
    "train_weights= None\n",
    "\n",
    "class LogSumExpPooling(Layer):\n",
    "\n",
    "    def call(self, x):\n",
    "        # could be axis 0 or 1\n",
    "        return tf.reduce_logsumexp(x, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:1]+input_shape[2:]\n",
    "\n",
    "def get_conv_stack(input_layer, filters, kernel_sizes, activation, kernel_l2_regularization, dropout_rate):\n",
    "    layers = [Conv1D(activation=activation, padding='same', strides=1, filters=filters, kernel_size = size,\n",
    "                kernel_regularizer=regularizers.l2(kernel_l2_regularization))(input_layer) for size in kernel_sizes]\n",
    "    if (len(layers) <= 0):\n",
    "        return input_layer\n",
    "    elif (len(layers) == 1):\n",
    "        return Dropout(dropout_rate, noise_shape=None, seed=None)(layers[0])\n",
    "    else:\n",
    "        return Dropout(dropout_rate, noise_shape=None, seed=None)(concatenate(layers))\n",
    "\n",
    "def get_model(embeddings=True, dilated_convs=False):\n",
    "    params = {k:v for k,v in locals().iteritems() if k!='weights'}\n",
    "    x = Input(shape=(MAX_CHORDS,NUM_NOTES), dtype='float32')\n",
    "    if embeddings:\n",
    "        y1 = Dense(NUM_DIM, activation='linear', use_bias=False, weights=[M1], trainable=False)(x)\n",
    "    else:\n",
    "        y1 = x\n",
    "    y2 = get_conv_stack(y1, 5, range(1,4), 'relu', 0.00001, 0.5)\n",
    "    y3 = GlobalMaxPool1D()(y2)\n",
    "    y = Dense(MAX_LABELS, activation='sigmoid')(y3)\n",
    "    model = Model(x, y)\n",
    "    adam = Adam(lr=c.lr)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=c.metrics)\n",
    "    return (model, params)\n",
    "\n",
    "def load_embeddings(embeddings_path='/home/yg2482/code/chord2vec/data/chord2vec_199.npz'):\n",
    "    logger.debug('loading embeddings from: '+embeddings_path)\n",
    "    global M1, M2, W, b2\n",
    "    npzf = np.load(embeddings_path)\n",
    "    M1 = npzf['wM1']\n",
    "    M2 = npzf['wM2']\n",
    "    W = npzf['wW']\n",
    "    b2 = npzf['bM2']\n",
    "\n",
    "def indices2multihot(x, r, dtype=np.float32):\n",
    "    v = np.zeros(r, dtype=dtype)\n",
    "    # x should belong to [1, 88]\n",
    "    x = filter(lambda x: x>0, x)\n",
    "    x = filter(lambda x: x<=r, x)\n",
    "    # decrease x to make in [0, 87]\n",
    "    x = map(lambda x: int(x-1), x)\n",
    "    v[x] = 1\n",
    "    return v\n",
    "\n",
    "def square3D(x, maxlen=None, dtype=np.float32):\n",
    "    if maxlen is None:\n",
    "        maxlen = []\n",
    "        maxlen.append(len(x))\n",
    "        maxlen.append(max([0]+[len(song) for song in x]))\n",
    "        maxlen.append(max([0]+[max([0]+[len(chord) for chord in song]) for song in x]))\n",
    "\n",
    "    x_np = np.zeros(maxlen, dtype=dtype)\n",
    "\n",
    "    for i in range(maxlen[0]):\n",
    "        for j in range(maxlen[1]):\n",
    "            for k in range(maxlen[2]):\n",
    "                try:\n",
    "                    x_np[i][j][k] = x[i][j][k]\n",
    "                except IndexError:\n",
    "                    break\n",
    "    return x_np\n",
    "\n",
    "def multihot3D(x, r, maxlen=None, dtype=np.float32):\n",
    "    f1D = lambda chord: indices2multihot(chord,r,dtype)\n",
    "    f2D = lambda song:map(f1D, song)\n",
    "    x_mh = map(f2D, x)\n",
    "    return x_mh\n",
    "    # return square3D(x_mh, maxlen=maxlen, dtype=dtype)\n",
    "\n",
    "def filter_index(X,y,idx):    \n",
    "    idxs = map(lambda x:x[idx]!=1,y)\n",
    "    filter_indices = lambda data : [x for i,x in enumerate(data) if idxs[i]]\n",
    "    return filter_indices(X), filter_indices(y)\n",
    "\n",
    "def filter_majority_index():\n",
    "    global train, y_train, test, y_test, valid, y_valid\n",
    "    most_common_index = np.argmax(y_train.sum(axis=0))\n",
    "    (train, y_train) = filter_index(train, y_train, most_common_index)\n",
    "    (test, y_test) = filter_index(test, y_test, most_common_index)\n",
    "    (valid, y_valid) = filter_index(valid, y_valid, most_common_index)\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    y_valid = np.array(y_valid)\n",
    "    \n",
    "\n",
    "import random\n",
    "\n",
    "def shuffle_train_valid(xt, yt, xv, yv):\n",
    "    l1 = len(yt)\n",
    "    l2 = len(yv)\n",
    "    assert (l1 == len(xt))\n",
    "    assert (l2 == len(xv))\n",
    "    c = zip(xt+xv, yt+yv)\n",
    "    random.shuffle(c)\n",
    "    xtv, ytv = zip(*c)\n",
    "    xt = xtv[:l1]\n",
    "    xv = xtv[l1:]\n",
    "    yt = ytv[:l1]\n",
    "    yv = ytv[l1:]    \n",
    "    return (xt, yt, xv, yv)\n",
    "\n",
    "def log_about_data():\n",
    "    logging.debug( 'train:\\n'+util.about(train, SINGLE_LINE=True) )\n",
    "    logging.debug( 'valid:\\n'+util.about(valid, SINGLE_LINE=True) )\n",
    "    logging.debug( 'test:\\n'+util.about(test, SINGLE_LINE=True) )\n",
    "    logging.debug( 'y_train:\\n'+util.about(y_train, SINGLE_LINE=True) )\n",
    "    logging.debug( 'y_valid:\\n'+util.about(y_valid, SINGLE_LINE=True) )\n",
    "    logging.debug( 'y_test:\\n'+util.about(y_test, SINGLE_LINE=True) )\n",
    "    \n",
    "    \n",
    "def load_data(x_datapath='data/X.pickle', y_datapath='data/y.pickle', cut=1.0,\n",
    "        load_train=True, train_params_path='data/train_params.npz'):\n",
    "    '''\n",
    "    x_datapath : path for X.pickle\n",
    "    y_datapath : path for y.pickle\n",
    "    cut : fraction in [0.0, 1.0] to load less data if required.\n",
    "    '''\n",
    "    global data, train, test, valid, MAX_CHORDS\n",
    "    global labels, y_train, y_test, y_valid, MAX_LABELS, index2label, labels2index\n",
    "    global train_weights\n",
    "\n",
    "    logger.debug('loading data from: '+x_datapath)\n",
    "    data = cPickle.load(open(x_datapath))\n",
    "\n",
    "    logger.debug('loading labels from: '+y_datapath)\n",
    "    labels = cPickle.load(open(y_datapath))\n",
    "\n",
    "    logging.debug('shuffling train and valid data and labels')\n",
    "    (data['train'], labels['train'], data['valid'], labels['valid'] ) = \\\n",
    "        shuffle_train_valid( data['train'], labels['train'], data['valid'], labels['valid'] )\n",
    "\n",
    "    train = data['train'] if load_train else None\n",
    "    test = data['test']\n",
    "    valid = data['valid']\n",
    "    \n",
    "    if(cut<1.0):\n",
    "        cutf = lambda x, c: x[:int(len(x)*cut)]\n",
    "        train = cutf(train, cut)\n",
    "        valid = cutf(valid, cut)\n",
    "        test = cutf(test, cut)\n",
    "        data2 = {'train':train, 'valid':valid, 'test':test}\n",
    "        cPickle.dump(data2, open(x_datapath+str(cut)+'.pickle', 'w'))\n",
    "\n",
    "    if(cut<1.0):\n",
    "        cutf = lambda x, c: x[:int(len(x)*cut)]\n",
    "        train = cutf(labels['train'], cut)\n",
    "        valid = cutf(labels['valid'], cut)\n",
    "        test = cutf( labels['test'], cut)\n",
    "        labels2 = {'train':train, 'valid':valid, 'test':test}\n",
    "        cPickle.dump(labels2, open(y_datapath+str(cut)+'.pickle', 'w'))\n",
    "        \n",
    "    train = multihot3D(train, NUM_NOTES) if load_train else None\n",
    "    test  = multihot3D(test, NUM_NOTES)\n",
    "    valid = multihot3D(valid, NUM_NOTES)\n",
    "    maxlen2D = lambda x : max([len(s) for s in x])\n",
    "    MAX_CHORDS = max( map(maxlen2D, [train, test, valid]))\n",
    "    # TODO: NORMALIZE!!!\n",
    "\n",
    "\n",
    "    s = Counter()\n",
    "    for k,v in labels.iteritems():\n",
    "        for y in v:\n",
    "            s[y]+=1\n",
    "\n",
    "\n",
    "    l = list(enumerate(s.keys()))\n",
    "    _index2label = {k:v for k,v in l}\n",
    "    index2label =  lambda x : _index2label[x]\n",
    "    _labels2index = {v:k for k,v in l}\n",
    "    labels2index = lambda x : _labels2index[x]\n",
    "\n",
    "    most_common_index = labels2index(s.most_common(n=1)[0][0])\n",
    "\n",
    "    MAX_LABELS = len(_labels2index)\n",
    "\n",
    "    y_train = to_categorical(map(labels2index, labels['train']), MAX_LABELS)\n",
    "    y_test = to_categorical(map(labels2index, labels['test']), MAX_LABELS)\n",
    "    y_valid = to_categorical(map(labels2index, labels['valid']), MAX_LABELS)\n",
    "    train_weights = dict(enumerate(np.load(train_params_path)['train_weights']))\n",
    "        \n",
    "    logging.debug( 'train:\\n'+util.about(train, SINGLE_LINE=True) )\n",
    "    logging.debug( 'valid:\\n'+util.about(valid, SINGLE_LINE=True) )\n",
    "    logging.debug( 'test:\\n'+util.about(test, SINGLE_LINE=True) )\n",
    "    logging.debug( 'y_train:\\n'+util.about(y_train, SINGLE_LINE=True) )\n",
    "    logging.debug( 'y_valid:\\n'+util.about(y_valid, SINGLE_LINE=True) )\n",
    "    logging.debug( 'y_test:\\n'+util.about(y_test, SINGLE_LINE=True) )\n",
    "\n",
    "class DataManager():\n",
    "    def __init__(self, inputs, targets, batch_size=128, maxepochs=10, transforms=lambda x:x):\n",
    "        self.datasize = len(inputs)\n",
    "        assert self.datasize == len(targets), 'size of targets should be the same as inputs'\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.batch_size = batch_size\n",
    "        self.maxepochs = maxepochs\n",
    "        self.num_batches = int(math.ceil(float(self.datasize)/batch_size))\n",
    "        if(callable(transforms)):\n",
    "            transforms = [transforms, transforms]\n",
    "        assert type(transforms)==list, 'transforms should be a *callable* or *list* of two callables'\n",
    "        assert len(transforms)==2, 'transforms should be a callable or list of *two* callables'\n",
    "        assert callable(transforms[0]) & callable(transforms[0]), 'transforms should be a callable or list of two *callables*'\n",
    "        self.inputs_transform = transforms[0]\n",
    "        self.targets_transform = transforms[1]\n",
    "        logger.info('created a DataManager for batch_size: {}, maxepochs: {}, num_batches: {}'.format(batch_size, maxepochs, self.num_batches))\n",
    "\n",
    "    def batch_generator(self):\n",
    "        for epoch in range(self.maxepochs):\n",
    "            for i in range(self.num_batches):\n",
    "                logger.debug('loading batch {} of {}, epoch {}'.format(i, self.num_batches, epoch))\n",
    "                start = i*self.batch_size\n",
    "                end = (i+1)*self.batch_size\n",
    "                inputs_batch =  self.inputs_transform(self.inputs[start:end])\n",
    "                targets_batch =  self.targets_transform(self.targets[start:end])\n",
    "                yield (inputs_batch, targets_batch)\n",
    "\n",
    "def save_history(history, dirpath):\n",
    "    with open(dirpath+'/training.json', 'w') as f:\n",
    "        json.dump(history.params, f, indent=2)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(history.history)\n",
    "    df.to_csv(dirpath+'/history.csv')\n",
    "    i = df.loc[:, c.monitor].argmax()\n",
    "\n",
    "    for m in c.metrics + ['loss']:\n",
    "        plot_metric(df, m, i, dirpath)\n",
    "\n",
    "    return\n",
    "\n",
    "def run_experiment(**kwargs):    \n",
    "    model, params = get_model( kwargs['embeddings'] )\n",
    "    hyperparams = fill_dict(params, kwargs)\n",
    "    \n",
    "    transforms = [lambda x:sequence.pad_sequences(x, MAX_CHORDS), lambda y:y]\n",
    "    dm_train = DataManager(train, y_train, batch_size=c.batch_size, maxepochs=c.epochs+1, transforms=transforms)\n",
    "    dm_valid = DataManager(valid, y_valid, batch_size=c.batch_size, maxepochs=100*c.epochs+1, transforms=transforms)\n",
    "    \n",
    "    with get_archiver(datadir='data/models') as a1, get_archiver(datadir='data/results') as a:\n",
    "\n",
    "        with open(a.getFilePath('hyperparameters.json'), 'w') as f:\n",
    "            json.dump(hyperparams, f, indent=2)\n",
    "\n",
    "        with open(a.getFilePath('model.json'), 'w') as f:\n",
    "            f.write(model.to_json(indent=2))\n",
    "\n",
    "        stdout = sys.stdout\n",
    "        with open(a.getFilePath('summary.txt'), 'w') as sys.stdout:\n",
    "            model.summary()\n",
    "        sys.stdout = stdout\n",
    "\n",
    "        plot_model(model, to_file=a.getFilePath('model.png'), show_shapes=True, show_layer_names=True)\n",
    "\n",
    "        earlystopping = EarlyStopping(monitor=c.monitor, patience=c.patience, verbose=0, mode=c.monitor_objective)\n",
    "        modelpath = a1.getFilePath('weights.h5')\n",
    "        csvlogger = CSVLogger(a.getFilePath('logger.csv'))\n",
    "        modelcheckpoint = ModelCheckpoint(modelpath, monitor=c.monitor, save_best_only=True, verbose=0, mode=c.monitor_objective)\n",
    "        logger.info('starting training')\n",
    "        logger.info(str((dm_train.num_batches, dm_valid.num_batches)))\n",
    "        h = model.fit_generator(generator=dm_train.batch_generator(), steps_per_epoch=dm_train.num_batches, epochs=c.epochs,\n",
    "                        validation_data=dm_valid.batch_generator(), validation_steps=dm_valid.num_batches,\n",
    "                        callbacks=[earlystopping, modelcheckpoint, csvlogger], class_weight=train_weights )\n",
    "    \n",
    "        save_history(h, a.getDirPath())\n",
    "\n",
    "def main():\n",
    "    commit_hash = save_code()\n",
    "    embeddings_path = '/home/yg2482/code/chord2vec/data/chord2vec_199.npz'\n",
    "    x_datapath='data/X.001.pickle'\n",
    "    y_datapath='data/y.001.pickle'\n",
    "    load_embeddings(embeddings_path=embeddings_path)\n",
    "    load_data(x_datapath=x_datapath, y_datapath=y_datapath)\n",
    "    run_experiment(**locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "embeddings_path = '../data2/chord2vec_30hr.npz'\n",
    "x_datapath='../data2/X.pickle0.001.pickle'\n",
    "y_datapath='../data2/y.pickle0.001.pickle'\n",
    "\n",
    "x_datapath='../data2/X.pickle'\n",
    "y_datapath='../data2/y.pickle'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 00:57:28,646:DEBUG:__main__:MainThread:line 74: loading embeddings from: ../data2/chord2vec_30hr.npz\n"
     ]
    }
   ],
   "source": [
    "load_embeddings(embeddings_path=embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 00:57:32,509:DEBUG:__main__:MainThread:line 169: loading data from: ../data2/X.pickle\n",
      "2017-05-06 00:59:11,044:DEBUG:__main__:MainThread:line 172: loading labels from: ../data2/y.pickle\n",
      "2017-05-06 00:59:11,083:DEBUG:root:MainThread:line 175: shuffling train and valid data and labels\n"
     ]
    }
   ],
   "source": [
    "load_data(x_datapath=x_datapath, y_datapath=y_datapath, train_params_path='../data2/train_params.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_about_data()\n",
    "filter_majority_index()\n",
    "log_about_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shuffle_train_valid?\n",
    "# log_about_data()\n",
    "# (train, y_train, valid, y_valid) = shuffle_train_valid(train, y_train, valid, y_valid)\n",
    "# log_about_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print util.about(a, SINGLE_LINE=True)\n",
    "print util.about(b, SINGLE_LINE=True)\n",
    "print y_test.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "t1[[3,7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "util.about?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ys = cPickle.load(open('../data2/y.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = Counter()\n",
    "for k,v in ys.iteritems():\n",
    "    for y in v:\n",
    "        s[y]+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
