{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level = logging.INFO , format=\n",
    "        '%(asctime)s:%(levelname)s:%(name)s:%(threadName)s:line %(lineno)d: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import cPickle\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Embedding, Conv1D, GlobalMaxPool1D, Dense, GlobalAvgPool1D, Dropout\n",
    "from keras.layers import concatenate\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import sequence\n",
    "from keras import regularizers\n",
    "from keras.engine.topology import Layer\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "\n",
    "from util import plot_model, plot_metric, save_code, fill_dict\n",
    "from util.archiver import get_archiver\n",
    "import config as c\n",
    "\n",
    "MAX_CHORDS = None\n",
    "MAX_LABELS = None\n",
    "NUM_NOTES = 88\n",
    "NUM_DIM = 1024\n",
    "\n",
    "M1 = M2 = W = b2 = None\n",
    "data= train= test= valid= MAX_CHORDS = None\n",
    "labels= y_train= y_test= y_valid= MAX_LABELS= index2label= labels2index = None\n",
    "train_weights= None\n",
    "\n",
    "class LogSumExpPooling(Layer):\n",
    "\n",
    "    def call(self, x):\n",
    "        # could be axis 0 or 1\n",
    "        return tf.reduce_logsumexp(x, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:1]+input_shape[2:]\n",
    "\n",
    "def get_conv_stack(input_layer, filters, kernel_sizes, activation, kernel_l2_regularization, dropout_rate):\n",
    "    layers = [Conv1D(activation=activation, padding='same', strides=1, filters=filters, kernel_size = size,\n",
    "                kernel_regularizer=regularizers.l2(kernel_l2_regularization))(input_layer) for size in kernel_sizes]\n",
    "    if (len(layers) <= 0):\n",
    "        return input_layer\n",
    "    elif (len(layers) == 1):\n",
    "        return Dropout(dropout_rate, noise_shape=None, seed=None)(layers[0])\n",
    "    else:\n",
    "        return Dropout(dropout_rate, noise_shape=None, seed=None)(concatenate(layers))\n",
    "\n",
    "def get_model(embeddings=True, dilated_convs=False):\n",
    "    params = {k:v for k,v in locals().iteritems() if k!='weights'}\n",
    "    x = Input(shape=(MAX_CHORDS,NUM_NOTES), dtype='float32')\n",
    "    if embeddings:\n",
    "        y1 = Dense(NUM_DIM, activation='linear', use_bias=False, weights=[M1], trainable=False)(x)\n",
    "    else:\n",
    "        y1 = x\n",
    "    y2 = get_conv_stack(y1, 5, range(1,4), 'relu', 0.00001, 0.5)\n",
    "    y3 = GlobalMaxPool1D()(y2)\n",
    "    y = Dense(MAX_LABELS, activation='sigmoid')(y3)\n",
    "    model = Model(x, y)\n",
    "    adam = Adam(lr=c.lr)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=c.metrics)\n",
    "    return (model, params)\n",
    "\n",
    "def load_embeddings(embeddings_path='/home/yg2482/code/chord2vec/data/chord2vec_199.npz'):\n",
    "    logger.debug('loading embeddings from: '+embeddings_path)\n",
    "    global M1, M2, W, b2\n",
    "    npzf = np.load(embeddings_path)\n",
    "    M1 = npzf['wM1']\n",
    "    M2 = npzf['wM2']\n",
    "    W = npzf['wW']\n",
    "    b2 = npzf['bM2']\n",
    "\n",
    "def indices2multihot(x, r, dtype=np.float32):\n",
    "    v = np.zeros(r, dtype=dtype)\n",
    "    # x should belong to [1, 88]\n",
    "    x = filter(lambda x: x>0, x)\n",
    "    x = filter(lambda x: x<=r, x)\n",
    "    # decrease x to make in [0, 87]\n",
    "    x = map(lambda x: int(x-1), x)\n",
    "    v[x] = 1\n",
    "    return v\n",
    "\n",
    "def square3D(x, maxlen=None, dtype=np.float32):\n",
    "    if maxlen is None:\n",
    "        maxlen = []\n",
    "        maxlen.append(len(x))\n",
    "        maxlen.append(max([0]+[len(song) for song in x]))\n",
    "        maxlen.append(max([0]+[max([0]+[len(chord) for chord in song]) for song in x]))\n",
    "\n",
    "    x_np = np.zeros(maxlen, dtype=dtype)\n",
    "\n",
    "    for i in range(maxlen[0]):\n",
    "        for j in range(maxlen[1]):\n",
    "            for k in range(maxlen[2]):\n",
    "                try:\n",
    "                    x_np[i][j][k] = x[i][j][k]\n",
    "                except IndexError:\n",
    "                    break\n",
    "    return x_np\n",
    "\n",
    "def multihot3D(x, r, maxlen=None, dtype=np.float32):\n",
    "    f1D = lambda chord: indices2multihot(chord,r,dtype)\n",
    "    f2D = lambda song:map(f1D, song)\n",
    "    x_mh = map(f2D, x)\n",
    "    return x_mh\n",
    "    # return square3D(x_mh, maxlen=maxlen, dtype=dtype)\n",
    "\n",
    "def load_data(x_datapath='data/X.pickle', y_datapath='data/y.pickle', cut=1.0,\n",
    "        load_train=True, train_params_path='data/train_params.npz', filter_majority=False):\n",
    "    '''\n",
    "    x_datapath : path for X.pickle\n",
    "    y_datapath : path for y.pickle\n",
    "    cut : fraction in [0.0, 1.0] to load less data if required.\n",
    "    '''\n",
    "    global data, train, test, valid, MAX_CHORDS\n",
    "    global labels, y_train, y_test, y_valid, MAX_LABELS, index2label, labels2index\n",
    "    global train_weights\n",
    "\n",
    "    logger.debug('loading data from: '+x_datapath)\n",
    "    data = cPickle.load(open(x_datapath))\n",
    "    train = data['train'] if load_train else None\n",
    "    test, valid = data['test'], data['valid']\n",
    "    if(cut<1.0):\n",
    "        cutf = lambda x, c: x[:int(len(x)*cut)]\n",
    "        train = cutf(train, cut)\n",
    "        valid = cutf(valid, cut)\n",
    "        test = cutf(test, cut)\n",
    "        data2 = {'train':train, 'valid':valid, 'test':test}\n",
    "        cPickle.dump(data2, open(x_datapath+str(cut)+'.pickle', 'w'))\n",
    "        \n",
    "    train = multihot3D(train, NUM_NOTES) if load_train else None\n",
    "    test  = multihot3D(test, NUM_NOTES)\n",
    "    valid = multihot3D(valid, NUM_NOTES)\n",
    "    maxlen2D = lambda x : max([len(s) for s in x])\n",
    "    MAX_CHORDS = max( map(maxlen2D, [train, test, valid]))\n",
    "    # TODO: NORMALIZE!!!\n",
    "\n",
    "    logger.debug('loading labels from: '+y_datapath)\n",
    "    labels = cPickle.load(open(y_datapath))\n",
    "    if(cut<1.0):\n",
    "        cutf = lambda x, c: x[:int(len(x)*cut)]\n",
    "        train = cutf(labels['train'], cut)\n",
    "        valid = cutf(labels['valid'], cut)\n",
    "        test = cutf( labels['test'], cut)\n",
    "        labels2 = {'train':train, 'valid':valid, 'test':test}\n",
    "        cPickle.dump(labels2, open(y_datapath+str(cut)+'.pickle', 'w'))\n",
    "\n",
    "    s = set()\n",
    "    for k,v in labels.iteritems():\n",
    "        for y in v:\n",
    "            s.add(y)\n",
    "    l = list(enumerate(s))\n",
    "    _index2label = {k:v for k,v in l}\n",
    "    index2label =  lambda x : _index2label[x]\n",
    "    _labels2index = {v:k for k,v in l}\n",
    "    labels2index = lambda x : _labels2index[x]\n",
    "\n",
    "    MAX_LABELS = len(_labels2index)\n",
    "\n",
    "    y_train = to_categorical(map(labels2index, labels['train']), MAX_LABELS)\n",
    "    y_test = to_categorical(map(labels2index, labels['test']), MAX_LABELS)\n",
    "    y_valid = to_categorical(map(labels2index, labels['valid']), MAX_LABELS)\n",
    "    train_weights = dict(enumerate(np.load(train_params_path)['train_weights']))\n",
    "    \n",
    "    if(filter_majority):\n",
    "        def filter_majority_function(X,y):\n",
    "            idxs = map(lambda x:x[1]==1,y)\n",
    "            filter_indices = lambda data : [x for i,x in enumerate(data) if i in idxs]\n",
    "            return filter_indices(X), filter_indices(y)\n",
    "        (train, y_train) = filter_majority_function(train, y_train)\n",
    "        (test, y_test) = filter_majority_function(test, y_test)\n",
    "        (valid, y_valid) = filter_majority_function(valid, y_valid)\n",
    "\n",
    "class DataManager():\n",
    "    def __init__(self, inputs, targets, batch_size=128, maxepochs=10, transforms=lambda x:x):\n",
    "        self.datasize = len(inputs)\n",
    "        assert self.datasize == len(targets), 'size of targets should be the same as inputs'\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.batch_size = batch_size\n",
    "        self.maxepochs = maxepochs\n",
    "        self.num_batches = int(math.ceil(float(self.datasize)/batch_size))\n",
    "        if(callable(transforms)):\n",
    "            transforms = [transforms, transforms]\n",
    "        assert type(transforms)==list, 'transforms should be a *callable* or *list* of two callables'\n",
    "        assert len(transforms)==2, 'transforms should be a callable or list of *two* callables'\n",
    "        assert callable(transforms[0]) & callable(transforms[0]), 'transforms should be a callable or list of two *callables*'\n",
    "        self.inputs_transform = transforms[0]\n",
    "        self.targets_transform = transforms[1]\n",
    "        logger.info('created a DataManager for batch_size: {}, maxepochs: {}, num_batches: {}'.format(batch_size, maxepochs, self.num_batches))\n",
    "\n",
    "    def batch_generator(self):\n",
    "        for epoch in range(self.maxepochs):\n",
    "            for i in range(self.num_batches):\n",
    "                logger.debug('loading batch {} of {}, epoch {}'.format(i, self.num_batches, epoch))\n",
    "                start = i*self.batch_size\n",
    "                end = (i+1)*self.batch_size\n",
    "                inputs_batch =  self.inputs_transform(self.inputs[start:end])\n",
    "                targets_batch =  self.targets_transform(self.targets[start:end])\n",
    "                yield (inputs_batch, targets_batch)\n",
    "\n",
    "def save_history(history, dirpath):\n",
    "    with open(dirpath+'/training.json', 'w') as f:\n",
    "        json.dump(history.params, f, indent=2)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(history.history)\n",
    "    df.to_csv(dirpath+'/history.csv')\n",
    "    i = df.loc[:, c.monitor].argmax()\n",
    "\n",
    "    for m in c.metrics + ['loss']:\n",
    "        plot_metric(df, m, i, dirpath)\n",
    "\n",
    "    return\n",
    "\n",
    "def run_experiment(**kwargs):    \n",
    "    model, params = get_model( kwargs['embeddings'] )\n",
    "    hyperparams = fill_dict(params, kwargs)\n",
    "    \n",
    "    transforms = [lambda x:sequence.pad_sequences(x, MAX_CHORDS), lambda y:y]\n",
    "    dm_train = DataManager(train, y_train, batch_size=c.batch_size, maxepochs=c.epochs+1, transforms=transforms)\n",
    "    dm_valid = DataManager(valid, y_valid, batch_size=c.batch_size, maxepochs=100*c.epochs+1, transforms=transforms)\n",
    "    \n",
    "    with get_archiver(datadir='data/models') as a1, get_archiver(datadir='data/results') as a:\n",
    "\n",
    "        with open(a.getFilePath('hyperparameters.json'), 'w') as f:\n",
    "            json.dump(hyperparams, f, indent=2)\n",
    "\n",
    "        with open(a.getFilePath('model.json'), 'w') as f:\n",
    "            f.write(model.to_json(indent=2))\n",
    "\n",
    "        stdout = sys.stdout\n",
    "        with open(a.getFilePath('summary.txt'), 'w') as sys.stdout:\n",
    "            model.summary()\n",
    "        sys.stdout = stdout\n",
    "\n",
    "        plot_model(model, to_file=a.getFilePath('model.png'), show_shapes=True, show_layer_names=True)\n",
    "\n",
    "        earlystopping = EarlyStopping(monitor=c.monitor, patience=c.patience, verbose=0, mode=c.monitor_objective)\n",
    "        modelpath = a1.getFilePath('weights.h5')\n",
    "        csvlogger = CSVLogger(a.getFilePath('logger.csv'))\n",
    "        modelcheckpoint = ModelCheckpoint(modelpath, monitor=c.monitor, save_best_only=True, verbose=0, mode=c.monitor_objective)\n",
    "        logger.info('starting training')\n",
    "        logger.info(str((dm_train.num_batches, dm_valid.num_batches)))\n",
    "        h = model.fit_generator(generator=dm_train.batch_generator(), steps_per_epoch=dm_train.num_batches, epochs=c.epochs,\n",
    "                        validation_data=dm_valid.batch_generator(), validation_steps=dm_valid.num_batches,\n",
    "                        callbacks=[earlystopping, modelcheckpoint, csvlogger], class_weight=train_weights )\n",
    "    \n",
    "        save_history(h, a.getDirPath())\n",
    "\n",
    "def main():\n",
    "    commit_hash = save_code()\n",
    "    embeddings_path = '/home/yg2482/code/chord2vec/data/chord2vec_199.npz'\n",
    "    x_datapath='data/X.001.pickle'\n",
    "    y_datapath='data/y.001.pickle'\n",
    "    load_embeddings(embeddings_path=embeddings_path)\n",
    "    load_data(x_datapath=x_datapath, y_datapath=y_datapath)\n",
    "    run_experiment(**locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_path = '../data2/chord2vec_30hr.npz'\n",
    "x_datapath='../data2/X.pickle0.001.pickle'\n",
    "y_datapath='../data2/y.pickle0.001.pickle'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_embeddings(embeddings_path=embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load_data('../data2/X.pickle', '../data2/y.pickle', filter_majority=True,\n",
    "          train_params_path='../data2/train_params.npz',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(y_train), len(train)\n",
    "print len(y_valid), len(valid)\n",
    "print len(y_test), len(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-05 01:33:13,028:INFO:__main__:MainThread:line 198: created a DataManager for batch_size: 256, maxepochs: 201, num_batches: 1\n",
      "2017-05-05 01:33:13,029:INFO:__main__:MainThread:line 198: created a DataManager for batch_size: 256, maxepochs: 20001, num_batches: 1\n",
      "2017-05-05 01:33:13,242:INFO:__main__:MainThread:line 250: starting training\n",
      "2017-05-05 01:33:13,244:INFO:__main__:MainThread:line 251: (1, 1)\n",
      "2017-05-05 01:33:13,563:INFO:util.archiver:MainThread:line 41: archived directory: /home/yg2482/code/music-styles/src/data/results/archive/20170505_013313.tar\n",
      "2017-05-05 01:33:13,670:INFO:util.archiver:MainThread:line 41: archived directory: /home/yg2482/code/music-styles/src/data/models/archive/20170505_013313.tar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 arrays but instead got the following list of 2 arrays: [array([ 0.,  0.,  1.]), array([ 0.,  0.,  1.])]...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-f7febf1ce4b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../data2/chord2vec_30hr.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-b510d542767a>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m         h = model.fit_generator(generator=dm_train.batch_generator(), steps_per_epoch=dm_train.num_batches, epochs=c.epochs,\n\u001b[1;32m    253\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdm_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdm_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m                         callbacks=[earlystopping, modelcheckpoint, csvlogger], class_weight=train_weights )\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0msave_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetDirPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yg2482/.virtualenvs/fbnyc-yo/local/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yg2482/.virtualenvs/fbnyc-yo/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1874\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1875\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1876\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1878\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yg2482/.virtualenvs/fbnyc-yo/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1612\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m             check_batch_axis=True)\n\u001b[0m\u001b[1;32m   1615\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1616\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yg2482/.virtualenvs/fbnyc-yo/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1297\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m                                     exception_prefix='model target')\n\u001b[0m\u001b[1;32m   1300\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1301\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m/home/yg2482/.virtualenvs/fbnyc-yo/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     72\u001b[0m                                  \u001b[0;34m'the following list of '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                                  \u001b[0;34m' arrays: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                                  '...')\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 arrays but instead got the following list of 2 arrays: [array([ 0.,  0.,  1.]), array([ 0.,  0.,  1.])]..."
     ]
    }
   ],
   "source": [
    "run_experiment(embeddings = '../data2/chord2vec_30hr.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e1b99fe36d8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers, not list"
     ]
    }
   ],
   "source": [
    "t1[[3,7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
