Richard:
* introduce us and sections we will do

What Gen?
* aim of gen models
* examples a few of which we have seen before
-hmm text to speach
-generative/variational autoencoders in generating sentences from cont. space

Why useful?

Game theory
* explain mixed strategy equilibrium

Adversarial networks
* mixed strategy because otherwise adversary will learn the non mixed data point
* Yogesh will show why the equilibrium occours when generate data learns true list
* paper introduces the general concept of any G and D but limits its investigations to multilayer perceptrons

better than others:
* generate samples faster than fully visible belief networks (wavenet pixelrnn)
* no MC approximations to gradient Boltzmann machines - don't work for high dim
* vae optimize lower bound on log-likelyhood so generate blurry samples
* no makov chain (bolzmann gsn) unknown number of itterations through chain
worse:
* no guarantees that it will find nash equilibrium
* bad for descrete data

Loss fn
* in practice better because when G is poor D can reject all samples trivially
  so log(1-D) saturates and not sufficient gradient for G to learn

Algo
* first two are essentially create m generated data and sample m from the training set
* video shows a 2d example 
 - blue training data
 - red generated
 - red blue is the soft threshold of descriminator